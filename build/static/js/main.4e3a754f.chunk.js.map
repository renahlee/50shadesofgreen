{"version":3,"sources":["theme.ts","components/Module.tsx","components/Section.tsx","App.tsx","content/introduction.ts","content/methodology.ts","content/results.ts","serviceWorker.js","index.js","static/Figure3_15.png"],"names":["breakpoints","fontSizes","colors","black","grey","lightgrey","yellow","green","lightgreen","space","fonts","body","heading","monospace","fontWeights","bold","lineHeights","shadows","small","large","variants","text","buttons","primary","color","bg","Module","index","sections","py","theme","fontFamily","fontWeight","fontSize","display","pr","mr","sx","borderRight","toLowerCase","map","section","Section","image","imageCaption","imagePos","maxWidth","pt","mb","src","width","height","mt","t","source","ml","modules","imageDescription","Figure3_15","App","px","pb","module","id","Boolean","window","location","hostname","match","ReactDOM","render","StrictMode","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message","exports"],"mappings":"qMAAe,GACbA,YAAa,CAAC,OAAQ,OAAQ,QAC9BC,UAAW,CAAC,GAAI,GAAI,GAAI,GAAI,GAAI,GAAI,GAAI,IACxCC,OAAQ,CACNC,MAAO,OACPC,KAAM,UACNC,UAAW,UACXC,OAAQ,UACRC,MAAO,UACPC,WAAY,WAEdC,MAAO,CAAC,EAAG,EAAG,EAAG,GAAI,GAAI,GAAI,IAAK,KAClCC,MAAO,CACLC,KAAM,wBACNC,QAAS,aACTC,UAAW,oBAEbC,YAAa,CACXH,KAAM,IACNC,QAAS,IACTG,KAAM,KAERC,YAAa,CACXL,KAAM,IACNC,QAAS,MAEXK,QAAS,CACPC,MAAO,8BACPC,MAAO,gCAETC,SAAU,GACVC,KAAM,GACNC,QAAS,CACPC,QAAS,CACPC,MAAO,QACPC,GAAI,aCtBGC,EAAgC,SAAC,GAAD,IAC3Cd,EAD2C,EAC3CA,QACAe,EAF2C,EAE3CA,MACAC,EAH2C,EAG3CA,SAH2C,OAK3C,kBAAC,IAAD,CACEC,GAAI,GAEJ,kBAAC,IAAD,CACEL,MAAOM,EAAM5B,OAAOM,WACpBuB,WAAW,eACXC,WAAW,SACXC,SAAS,OACTC,QAAQ,gBAEG,IAAVP,GACI,kBAAC,IAAD,CACDH,MAAOM,EAAM5B,OAAOG,UACpB6B,QAAQ,eACRC,GAAI,EACJC,GAAI,EACJC,GAAI,CACFC,YAAY,aAAD,OAAeR,EAAM5B,OAAOG,aAEzCsB,GAEHf,EAAQ2B,eAEVX,EAASY,KAAI,SAACC,GAAD,OAAkB,kBAAC,EAC3BA,Q,iBC1BGC,EAAyB,SAAC,GAAD,IAAGC,EAAH,EAAGA,MAAOC,EAAV,EAAUA,aAAcC,EAAxB,EAAwBA,SAAUxB,EAAlC,EAAkCA,KAAlC,OACpC,oCA6BE,kBAAC,IAAD,CACEyB,SAAS,QACTC,GAAI,KAGAJ,GACW,UAAbE,GACA,kBAAC,IAAD,CACEG,GAAI,GAEJ,kBAAC,IAAD,CACEC,IAAKN,EACLO,MAAM,OACNC,OAAO,OACPf,GAAI,CAAC,EAAG,EAAG,KAEb,kBAAC,IAAD,CACEZ,MAAOM,EAAM5B,OAAOE,KACpB6B,SAAS,QACTmB,GAAI,GACJR,MAKFvB,GAAQA,EAAKmB,KAAI,SAACa,GAAD,OACjB,kBAAC,IAAD,CAAeC,OAAQD,SAKvBV,GACW,UAAbE,GACA,kBAAC,IAAD,CACEI,IAAKN,EACLO,MAAM,OACNC,OAAO,OACPI,GAAI,CAAC,EAAG,EAAG,GACXH,GAAI,CAAC,EAAG,EAAG,Q,iBCvEfI,EAAU,CCXqB,CACnC5C,QAAS,eACTgB,SAAU,CACR,CACEP,KAAM,CACJ,kaACA,uBACA,2kBAGJ,CACEsB,MAAO,OACPE,SAAU,OACVD,aAAc,sCACda,iBACE,qNAEJ,CACEpC,KAAM,CACJ,gCACA,4PACA,kwBACA,8eAGJ,CACEA,KAAM,CACJ,sCACA,kbACA,2kBACA,ocC9B4B,CAClCT,QAAS,cACTgB,SAAU,CACR,CACEP,KAAM,CACJ,2IACA,+BACA,kcACA,ocACA,0MAGA,ifAGJ,CACEsB,MAAO,MACPE,SAAU,OACVD,aAAc,4DAEhB,CACED,MAAO,MACPE,SAAU,OACVD,aAAc,oDAEhB,CACEvB,KAAM,CACJ,4YACA,waACA,6XACA,4LAEA,qDACA,+BACA,wPACA,oBACA,8cACA,slBACA,+BACA,krBACA,8jBACA,wOCxCwB,CAC9BT,QAAS,uBACTgB,SAAU,CACR,CACEP,KAAM,CACJ,+BACA,0NAGJ,CACEsB,MAAO,IACPC,aAAc,0CACda,iBACE,oNACFZ,SAAU,QAEZ,CACExB,KAAM,CACJ,iBACA,4OAGJ,CACEsB,MAAO,IACPC,aAAc,oCACda,iBACE,iFACFZ,SAAU,QAEZ,CACEF,MAAO,IACPC,aAAc,8BACda,iBACE,oHACFZ,SAAU,QAEZ,CACExB,KAAM,CACJ,sLAGJ,CACEsB,MAAO,IACPC,aAAc,oCACda,iBACE,+EACFZ,SAAU,QAEZ,CACEF,MAAO,IACPC,aAAc,8BACda,iBACE,wHACFZ,SAAU,QAEZ,CACEF,MAAO,IACPC,aAAc,qBACda,iBAAkB,6CAClBZ,SAAU,QAEZ,CACExB,KAAM,CACJ,iaACA,yBACA,mbAGJ,CACEsB,MAAO,IACPC,aAAc,oCACda,iBACE,4GACFZ,SAAU,QAEZ,CACEF,MAAO,IACPC,aAAc,8BACda,iBACE,6GAEJ,CACEpC,KAAM,CACJ,2aACA,6CACA,2PACA,4hBAGJ,CACEsB,MAAO,IACPC,aAAc,uBACda,iBACE,6FAEJ,CACEpC,KAAM,CACJ,oLACA,iBAGJ,CACEsB,MAAO,KACPC,aAAc,4CAEhB,CACED,MAAO,KACPC,aACE,wFAEJ,CACEvB,KAAM,CACJ,wVAGJ,CACEsB,MAAO,KACPC,aACE,wFAEJ,CACED,MAAO,KACPC,aAAc,8CAEhB,CACED,MAAO,KACPC,aAAc,wDAEhB,CACEvB,KAAM,CACJ,0QAGJ,CACEsB,M,OAAOe,EACPd,aAAc,uDACda,iBACE,8HAEJ,CACEd,MAAO,KACPC,aAAc,yDACda,iBACE,8GAEJ,CACEpC,KAAM,CACJ,yQAGJ,CACEsB,MAAO,KACPC,aAAc,sCACda,iBACE,sFAEJ,CACEd,MAAO,KACPC,aAAc,mCACda,iBACE,kEAEJ,CACEpC,KAAM,CACJ,+BACA,4BACA,4qBAGJ,CACEsB,MAAO,KACPC,aAAc,gCAEhB,CACED,MAAO,KACPC,aAAc,yCHhHLe,EA/CH,WACV,OAAO,kBAAC,IAAD,CAAe7B,MAAOA,GAC3B,kBAAC,IAAD,CAAK8B,GAAI,GACP,kBAAC,IAAD,CACEpC,MAAOM,EAAM5B,OAAOK,MACpBwB,WAAW,eACXE,SAAS,OACTD,WAAW,SACXe,GAAI,EACJc,GAAI,GANN,mCAUA,kBAAC,IAAD,CACErC,MAAOM,EAAM5B,OAAOE,KACpB6B,SAAS,QACT4B,GAAI,GAHN,4EAMCL,EAAQhB,KAAI,SAACsB,EAAQnC,GAAT,OACX,kBAAC,EAAD,CACEA,MAAOA,EACPf,QAASkD,EAAOlD,QAChBgB,SAAUkC,EAAOlC,eAIvB,yBAAKmC,GAAG,UACN,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,+BIjDcC,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCbNC,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,EAAD,OAEFC,SAASC,eAAe,SD0HpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,a,mBEzI5BrB,EAAOsB,QAAU,IAA0B,wC","file":"static/js/main.4e3a754f.chunk.js","sourcesContent":["export default {\n  breakpoints: [\"40em\", \"52em\", \"64em\"],\n  fontSizes: [12, 14, 16, 20, 24, 32, 48, 64],\n  colors: {\n    black: \"#000\",\n    grey: \"#838389\",\n    lightgrey: \"#F0F0F1\",\n    yellow: \"#FFBD41\",\n    green: \"#4A7856\",\n    lightgreen: \"#C8D5B9\"\n  },\n  space: [0, 4, 8, 16, 32, 64, 128, 256],\n  fonts: {\n    body: \"system-ui, sans-serif\",\n    heading: \"sans-serif\",\n    monospace: \"Menlo, monospace\"\n  },\n  fontWeights: {\n    body: 400,\n    heading: 700,\n    bold: 700\n  },\n  lineHeights: {\n    body: 1.5,\n    heading: 1.25\n  },\n  shadows: {\n    small: \"0 0 4px rgba(0, 0, 0, .125)\",\n    large: \"0 0 24px rgba(0, 0, 0, .125)\"\n  },\n  variants: {},\n  text: {},\n  buttons: {\n    primary: {\n      color: \"white\",\n      bg: \"primary\"\n    }\n  }\n};\n","import React from 'react'\nimport { Box, Heading, Text } from 'rebass'\nimport { Section } from \"./\"\nimport { SectionProps as TSection } from './Section'\nimport theme from '../theme'\n\nexport type ModuleProps = {\n  heading: string\n  index?: number\n  sections: TSection[]\n}\n\n\nexport const Module: React.FC<ModuleProps> = ({\n  heading,\n  index,\n  sections\n}) =>\n  <Box\n    py={3}\n  >\n    <Heading\n      color={theme.colors.lightgreen}\n      fontFamily=\"Nanum Gothic\"\n      fontWeight=\"normal\"\n      fontSize=\"3rem\"\n      display=\"inline-block\"\n    >\n      {index !== 0\n        && <Text\n          color={theme.colors.lightgrey}\n          display=\"inline-block\"\n          pr={2}\n          mr={3}\n          sx={{\n            borderRight: `1px solid ${theme.colors.lightgrey}`\n          }}\n        >{index}</Text>\n      }\n      {heading.toLowerCase()}</Heading>\n\n    {sections.map((section: any) => <Section\n      {...section}\n    >\n    </Section>)}\n  </Box>\n","import React from 'react'\nimport { Box, Flex, Heading, Image, Text } from 'rebass'\nimport ReactMarkdown from 'react-markdown'\nimport theme from '../theme'\n\nexport type SectionProps = {\n  image?: string\n  imagePos?: \"left\" | \"right\" | \"center\"\n  imageCaption?: string\n  imageDescription?: string\n  list?: string[]\n  subtitle?: string\n  text?: string[]\n  title?: string\n}\n\nexport const Section: React.FC<any> = ({ image, imageCaption, imagePos, text }) =>\n  <>\n    {/* {!!title &&\n      <Heading\n        color={theme.colors.yellow}\n        fontSize={3}\n        fontFamily=\"serif\"\n        fontWeight=\"normal\"\n        pt={4}\n      >\n        <ReactMarkdown\n          source={title}\n        />\n      </Heading>}\n\n    {\n      !!subtitle &&\n      <Heading\n        color={theme.colors.green}\n        fontSize={2}\n        fontFamily=\"serif\"\n        fontWeight=\"normal\"\n        pt={4}\n      >\n        <ReactMarkdown\n          source={subtitle}\n        />\n      </Heading>\n    } */}\n\n    <Box\n      maxWidth=\"40rem\"\n      pt={3}\n    >\n      {\n        !!image &&\n        imagePos !== \"right\" &&\n        <Box\n          mb={3}\n        >\n          <Image\n            src={image}\n            width=\"100%\"\n            height=\"100%\"\n            mr={[0, 0, 2]}\n          />\n          <Text\n            color={theme.colors.grey}\n            fontSize=\"0.8em\"\n            mt={1}\n          >{imageCaption}</Text>\n        </Box>\n      }\n\n      {\n        !!text && text.map((t: any) =>\n          <ReactMarkdown source={t} />\n        )\n      }\n\n      {\n        !!image &&\n        imagePos === \"right\" &&\n        <Image\n          src={image}\n          width=\"100%\"\n          height=\"100%\"\n          ml={[0, 0, 2]}\n          mt={[3, 0, 0]}\n        />\n      }\n    </Box>\n  </>\n","import React from 'react';\nimport { ThemeProvider } from 'emotion-theming'\nimport { Heading, Text, Box } from 'rebass'\nimport { Module } from './components'\nimport \"./App.css\"\nimport theme from './theme'\nimport {\n  Introduction,\n  Methodology,\n  Results\n} from './content'\n\n\nconst modules = [\n  Introduction,\n  Methodology,\n  Results\n] as const\n\nconst App = () => {\n  return <ThemeProvider theme={theme}>\n    <Box px={5}>\n      <Heading\n        color={theme.colors.green}\n        fontFamily=\"Nanum Gothic\"\n        fontSize=\"4rem\"\n        fontWeight=\"normal\"\n        pt={4}\n        pb={2}\n      >\n        50 shades of green üåø\n      </Heading>\n      <Text\n        color={theme.colors.grey}\n        fontSize=\"0.8em\"\n        pb={5}\n      >Samuel Elkind, Ollie Hsieh, Michael Liang, Nick Martucci, Matt Redington</Text>\n\n      {modules.map((module, index) =>\n        <Module\n          index={index}\n          heading={module.heading}\n          sections={module.sections}\n        />\n      )}\n    </Box>\n    <div id=\"leaves\">\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n    </div>\n  </ThemeProvider >\n}\n\nexport default App;\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nexport const Introduction: TModule = {\n  heading: \"Introduction\",\n  sections: [\n    {\n      text: [\n        \"Successful crop cultivation depends on weed control during the initial weeks of planting. Recently, area-saturation pesticide usage preferentially supplanted mechanical weed control but failed to completely eliminate weed problems.  As such, site-specific pesticide application is needed to reduce overall pesticide usage. Thus, rapidly identifying sparsely-distributed weeds is crucial for the agricultural sector.\",\n        \"## üå± Data\",\n        \"Researchers from University of Southern Denmark and Aarhus University publicly released a dataset to facilitate ML-based weed classification during nascent phases and intervention prior to their negative influence on cash crop growth. The data set comprises 5, 539 images of seedlings grown in consistent indoor conditions, taken at 2-3 day intervals over a 20-day period, labelled by species (3 crop, 9 weed species), and with a resolution of 10 pixel-per-millimeter. The dataset has been published on [Kaggle](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset/kernels).\"\n      ]\n    },\n    {\n      image: \"data\",\n      imagePos: \"left\",\n      imageCaption: \"12 Classes of Plants - Input Images\",\n      imageDescription:\n        \"Black-Grass, Charlock, Cleavers, Common Chickweed, Common Wheat, Fat Hen, Loose Silky-Bent, Maize, Scentless Mayweed, Shepherd‚Äôs Purse, Small-Flowered Cranesbill, Sugar Beet (left to right; top to bottom)\"\n    },\n    {\n      text: [\n        \"## üå± Previous work\",\n        \"Convolutional Neural Networks (CNNs) have been shown to be ideally suited for image classification tasks. Since the publication of the dataset on Kaggle, there have been several attempts at using CNNs to classify images from this dataset by species.\",\n        \"Alimboyong et. al. from the Technological Institute of the Philippines used extensive data augmentation to increase the size of the dataset to 118,750 images. 83,126 of these were applied to train a CNN with an AlexNet architecture through transfer learning from weights initialized on the ImageNet dataset. This model achieved 99.74% accuracy on the augmented test set. While impressive, this model took 99 hours to train, not including augmented dataset creation, making this approach infeasible for real-world applications. Since agricultural needs vary widely, a cheaper approach would allow each cultivator to produce a classification model trained on relevant crop and weed species in the environment in which the model will be applied; yielding better results.\",\n        \"Kaggle user ‚ÄúMarsh‚Äù takes the opposite approach. Citing the class imbalance present in the dataset, they seek to train an effective model by downsampling the over-represented plant species to 250 images per species. A custom architecture was used with randomly initialized weights. After 40 training epochs, this model correctly classified 79% of the test set. The smaller training set and light-weight architecture resulted in a drastically shorter training time of 400 seconds.\"\n      ]\n    },\n    {\n      text: [\n        \"## üå± Research objectives\",\n        \"Generally, the current practice for weed identification uses a CNN to classify images. The quality of the produced model depends heavily on the quality of the data set. Allunia highlights the danger of applying a model without examining how the model has extracted knowledge, especially when the data set is artificial and standardized. The lack of variation in our raw data set may yield a CNN model that does not generalize well.\",\n        \"In particular, target leakage of background pixels is a primary concern. Every seedling image is taken against a background of pebbles and/or a measuring stick. Different species of seedlings have different sizes at the same growth stage, meaning that pictures of smaller species will have a higher zoom level. A CNN can use the relative scale of the pebbles or measuring stick to classify the seedlings instead of using the features of the seedlings themselves. This is a problem because test data sets in the real world may have the seedlings against different types of backgrounds.\",\n        \"Another issue with this data set is an imbalance of class sizes. There are three crop species compared to nine weed species, and the number of images per species ranges from 253 to 762. The least represented species is common wheat, a cash crop whose correct classification is essential for industry. We add significant contributions to a combination of previous works to address these problems and produce a more accurate and generalized model.\"\n      ]\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nexport const Methodology: TModule = {\n  heading: \"Methodology\",\n  sections: [\n    {\n      text: [\n        \"For this project, we investigated both supervised and unsupervised learning techniques to support the classification of weeds and crops.\",\n        \"## Unsupervised segmentation\",\n        \"Prior work upon the first dataset iteration indicated that target leakage of background is a major problem.  Therefore, we decided to employ unsupervised learning to segment the plant leaves from the background before training our supervised learners.  Without ground truth values for the plant body within the original dataset, our goals for unsupervised segmentation were to extract clear-enough leaves for passing to the supervised classifiers.\",\n        \"Intuitively, clustering within color-space presented the best option, since the target is fairly different from the background.  Since image color information is a bounded, continuous space, we decided that hard assignment along the color-information axes (and not spatial axes) would be sufficient and much more performant.  Therefore, we decided upon k-means clustering as our segmentation technique.  However, this posed three research questions:\",\n        \"1. Which color space is best to quantize?\\n\" +\n          \"1. What is the optimal number of cluster centers to segment out the plant body?\\n\" +\n          \"1. How do we avoid mislabeling due to reflectance and illumination changes?\",\n        \"For the first research question, we compared plant body segmentation for the quantization of all color channels within RGB-space to the hue channel within HSV-space. To do so, we used the elbow method to generate a putative range of the number of clusters for each space, and then visually confirmed that segmentation would produce a cluster centroid corresponding to a plant body. Furthermore, we compared runtimes to determine which method was more performant, both in runtime and accuracy.\"\n      ]\n    },\n    {\n      image: \"hsv\",\n      imagePos: \"left\",\n      imageCaption: \"HSV Space Cylinder - The HSV model mapped to a cylinder.\"\n    },\n    {\n      image: \"rgb\",\n      imagePos: \"left\",\n      imageCaption: \"RGB Space Cube - The RGB model mapped to a cube.\"\n    },\n    {\n      text: [\n        \"In addition, there exists the need to automatically determine the optimal number of clusters upon which to quantize hue information in order to facilitate rapid segmentation.  To do so, we utilized the putative range of cluster numbers from the elbow method to calculate the average silhouette value for all clusters, retaining the number of clusters with the maximum average silhouette value.\",\n        \"However, hue-quantization presents the additional challenge of illumination changes affecting diffuse reflection of green hues off of white surfaces (See Figure 2.3).  To resolve this, we investigated two processes: (1) thresholding, which is a pre-process that removes all near-white pixels before segmentation, and (2) cascading segmentation, which implements two rounds of segmentation within alternating color spaces.\",\n        \"Once our images are pre-processed, we can implement k-means clustering upon just the relevant channels.  The resultant image can be processed with non-green suppression to extract the pixels corresponding to the plant body before passing the processed images to the supervised learners. In case multiple ‚Äúgreen‚Äù hues are retained, we implemented two different methods:\",\n        \"1. MaxGreen, which segments according to the most common green hue found in the image, **or**\\n\" +\n          \"1. AllGreen, which segments according to all possible green hues found as cluster centers.\",\n        \"Figure 2.3 shows the entire segmentation pipeline.\",\n        \"## Supervised classification\",\n        \"Given labels for the input images, as well as segmented versions of the images, we wished to build classification models to distinguish both between weeds and crops.  We compared a Random Forest to a Neural Network, which we will describe below.\",\n        \"### Random forest\",\n        \"Since quickly training models is a desirable feature of the model, it is important to research the performance difference in classification between Random Forest and Neural Networks.  In the investigation, Random Forest models were created for both binary classification between weeds and crops, as well as classification between the comprehensive list of labels.  Since images were segmented through k-means, models were also trained on the segmented images.\",\n        \"In order to train the Random Forest model, features need to be extracted that differentiate between images. Pixel color histograms were generated for the color images, as well as pixel intensities for grayscale images.  Bin values were then extracted as linear features.  In addition, multidimensional histograms using AND logic were created for the red, green, and blue-values as well:  One for all three together and two for red/green and blue/green.  For example, one bin would be for how many pixels have red, green and blue values between 0 and 10. The histograms can be seen plotted below:\",\n        \"## Neural network classifier\",\n        \"We aim to develop a CNN image classifier that yields accurate results without a resource or time-intensive training process; all while dealing with the issues inherent in this dataset. To address the class imbalance, we create a balanced dataset by undersampling all classes down to 253 images per species. We create a training set and a testing set using an 80/20 split. We use 80% of the training set for training and 20% for validation. These splits yield 1948 training images, 488 validation images, and 600 testing images. To compensate for this relatively small dataset, we use the Xception model architecture and use initial weights that have been trained on the ImageNet dataset.\",\n        \"The model was trained using the Adam optimizer with an initial learning rate of 0.0001 for 60 epochs. Training data was fed to the model in batches of 32 images. After each epoch, the validation accuracy was calculated. If the validation accuracy did not improve for three consecutive epochs, the learning rate was reduced by a factor of ‚Öï with a minimum possible learning rate of 0.000001. If the validation accuracy did not improve for 10 consecutive epochs, training was ended and the model was saved with the weights that yielded the highest validation accuracy.\",\n        \"This training procedure was repeated to train a model using the segmented images created by the process described above. The performance of these two models can be used to characterize the impact of background target leakage.\"\n      ]\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\nimport Figure3_15 from \"../static/Figure3_15.png\";\n\nexport const Results: TModule = {\n  heading: \"Results and Analysis\",\n  sections: [\n    {\n      text: [\n        \"## Unsupervised segmentation\",\n        \"In general, unsupervised segmentation produced high-quality results.  In the following sections, we will go over the specific results of our experiments, as well as discuss our strategies for problematic outliers.\"\n      ]\n    },\n    {\n      image: \"1\",\n      imageCaption: \"12 Classes of Plants - Segmented Images\",\n      imageDescription:\n        \"Black-Grass, Charlock, Cleavers, Common Chickweed, Common Wheat, Fat Hen, Loose Silky-Bent, Maize, Scentless Mayweed, Shepherd‚Äôs Purse, Small-Flowered Cranesbill, Sugar Beet (left to right; top to bottom)\",\n      imagePos: \"left\"\n    },\n    {\n      text: [\n        \"## RGB vs. HSV\",\n        \"Initial naive clustering within RGB-space allowed for determining the optimal number of centers using the elbow method which resulted in a consistent range of cluster numbers k=[3,6] for all weeds and cash crops within the dataset.\"\n      ]\n    },\n    {\n      image: \"2\",\n      imageCaption: \"Elbow Method for RGB Quantization\",\n      imageDescription:\n        \"These representative results indicate an optimal *k*-value in the range [3,6].\",\n      imagePos: \"left\"\n    },\n    {\n      image: \"3\",\n      imageCaption: \"Results of RGB Quantization\",\n      imageDescription:\n        \"The loss of the plant body and confusion with the background consistently occurs for plant bodies that are sparse\",\n      imagePos: \"left\"\n    },\n    {\n      text: [\n        \"Similarly, applying the elbow method to hue-quantization within HSV-space produced the same optimal number of cluster centers as with RGB-space, but with much lower loss values.\"\n      ]\n    },\n    {\n      image: \"4\",\n      imageCaption: \"Elbow Method for Hue Quantization\",\n      imageDescription:\n        \"These representative results indicate an optimal k-value in the range [3,6].\",\n      imagePos: \"left\"\n    },\n    {\n      image: \"5\",\n      imageCaption: \"Results of Hue Quantization\",\n      imageDescription:\n        \"The plant body is retained, as the hue information for the plant body is more likely to be a centroid for clustering.\",\n      imagePos: \"left\"\n    },\n    {\n      image: \"6\",\n      imageCaption: \"RGB/HSV Time Ratio\",\n      imageDescription: \"HSV quantization is multiple times faster.\",\n      imagePos: \"left\"\n    },\n    {\n      text: [\n        \"Segmentation using the ‚Äúoptimal‚Äù k-range for RGB-quantization assigns the plant body to background centroids for images in which plant body pixels are sparse.  This is particularly egregious for ‚Äúgrass‚Äù-type plant bodies (See Figure 3.2).  However, hue-quantization doesn‚Äôt suffer from these issues (See Figure 3.3).  Therefore, we decided to primarily use hue-quantization moving forward.\",\n        \"## Finding Optimal *k*\",\n        \"In order to determine the best k-value for each image, we calculate the silhouette score of each pixel with respect to its cluster label and determine the average over all observations.  This was conducted with respect to the hue feature.  Then, we pick the number of clusters associated with the silhouette score closest to 1.  In Figures 3.6 and 3.7, we plot each observation‚Äôs silhouette value within its assigned cluster.\"\n      ]\n    },\n    {\n      image: \"7\",\n      imageCaption: \"Representative Weed (Black Grass)\",\n      imageDescription:\n        \"Silhouette analysis of the image indicates a sparse plant body within the image with a best k-value of 3.\",\n      imagePos: \"left\"\n    },\n    {\n      image: \"8\",\n      imageCaption: \"Representative Crop (Maize)\",\n      imageDescription:\n        \"Silhouette analysis of the image indicates a sparse plant body within the image with a best k-value of 4.\"\n    },\n    {\n      text: [\n        \"We can see that both the shape of the curve, as well as the average value, correspond to the quality of the cluster assignment. This process requires downsampling the image to reasonable observation size (n <= 10&#x2074;), but leads to reasonable performance prior to segmentation. We can qualitatively assess the validity of our cluster number determination with segmentation results implemented with non-green suppression.\",\n        \"### Thresholding and cascaded segmentation\",\n        \"With the initial set of results from k-means, segmented images had varying levels of background and noise present. Given that k-means converges toward local minima, we employ thresholding to mitigate potential assignment of white pixels as centers.\",\n        \"Determined through tuning representative samples of 15 then 50 images, the white threshold values ranged from [120, 120, 120] to [180, 180, 180] and were suppressed as black pixels. Values chosen were largely influenced by illuminance, hue similarity between the background and leaves, and qualitative results from the representative samples. One might have assumed that the values were closer to [255, 255, 255] but as brightness is perceived in a relative fashion, the threshold optima point to underexposure or low light conditions.\"\n      ]\n    },\n    {\n      image: \"9\",\n      imageCaption: \"Sample (Black-grass)\",\n      imageDescription:\n        \"Original image, thresholding, initial k-means, k-means after thresholding (left to right)\"\n    },\n    {\n      text: [\n        \"As shown in Figure 3.8, thresholding effectively reduced problematic background pixels, particularly white pixels susceptible to adoption of hue characteristics via reflectance.\",\n        \"### Problems\"\n      ]\n    },\n    {\n      image: \"10\",\n      imageCaption: \"Background leakage due to hue similarity\"\n    },\n    {\n      image: \"11\",\n      imageCaption:\n        \"Absence of plant pixels due to hue similarity; inaccurate plant pixel identification\"\n    },\n    {\n      text: [\n        \"Other negative factors stemmed from image conditions, such as low light, underexposure, and blurriness from misfocus or poor image quality. The former two led to image clipping (loss of detail) whereas the latter produced masks with ridged edges and opened models to misinterpretation of background pixels with plant-like hues as features.\"\n      ]\n    },\n    {\n      image: \"12\",\n      imageCaption:\n        \"Sugar beet - clipping from low light/underexposure; loss of features from blurriness\"\n    },\n    {\n      image: \"13\",\n      imageCaption: \"Cleavers - ridged edges due to blurriness \"\n    },\n    {\n      image: \"14\",\n      imageCaption: \"Maize - ridged edges due to low light / low contrast\"\n    },\n    {\n      text: [\n        \"Considerations for improving segmentation results included normalizing image values and evaluating the ratio of plant to background pixels. However the inconsistency in image conditions and presence of features ultimately led to abandonment of these approaches.\"\n      ]\n    },\n    {\n      image: Figure3_15,\n      imageCaption: \"Maize - ridged edges due to low light / low contrast\",\n      imageDescription:\n        \"Blurry, zoomed-in, with container; blurry, zoomed-in, inter-plant occlusion;\\nzoomed-out, dense foliage; blurry, zoomed-in\"\n    },\n    {\n      image: \"16\",\n      imageCaption: \"Black-grass - variance in image content and conditions\",\n      imageDescription:\n        \"Zoomed-out, with white ruler; blurry, zoomed-in;\\nsparse plant pixels, with white ruler; blurry, zoomed-in\"\n    },\n    {\n      text: [\n        \"Similarly, an attempt to run images through two passes of k-means yielded mixed results. As shown in Figure 3.17 and 3.18, this approach proved capable of both successfully isolating plant pixels and faring worse than a single k-means pass without suppression.\"\n      ]\n    },\n    {\n      image: \"17\",\n      imageCaption: \"Cascading Segmentation: Black Grass\",\n      imageDescription:\n        \"Hue-quantization, followed by RGB-quantization successfully suppressed background.\"\n    },\n    {\n      image: \"18\",\n      imageCaption: \"Cascading Segmentation: Cleavers\",\n      imageDescription:\n        \"Hue-quantization, followed by RGB-quantization produced holes.\"\n    },\n    {\n      text: [\n        \"## Supervised classification\",\n        \"### Random forest results\",\n        \"Feature creation results in 2688 bins, most of which will not provide any information for the model, requiring a reduction in feature space. The top 200 features are selected using the ANOVA F-value between the label and the feature for a classification task. Due to the features just being different bins in different histograms, it is hard to distinguish which 200 bins are used because the function does not return anything that is labeled. The model itself will also return values for the importance of features but due to not being able to keep track of which bins are being used this information is not valuable. It takes approximately 9.22 seconds to extract the features.\"\n      ]\n    },\n    {\n      image: \"19\",\n      imageCaption: \"Representative RGB Histogram\"\n    },\n    {\n      image: \"20\",\n      imageCaption: \"Representative Grayscale Histogram\"\n    }\n  ]\n};\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n","module.exports = __webpack_public_path__ + \"static/media/Figure3_15.1be00a33.png\";"],"sourceRoot":""}