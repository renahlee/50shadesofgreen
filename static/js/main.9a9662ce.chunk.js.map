{"version":3,"sources":["theme.ts","components/Module.tsx","components/Section.tsx","content/introduction.ts","content/methodology.ts","App.tsx","content/results.ts","content/discussion.ts","content/conclusion.ts","content/sources.ts","serviceWorker.js","index.js","static/Figure1_1.png","static/Figure2_1.png","static/Figure2_2.png","static/Figure2_3.jpg","static/Figure2_4.png","static/Figure2_5.png","static/Figure3_1.png","static/Figure3_2.png","static/Figure3_3.png","static/Figure3_4.png","static/Figure3_5.png","static/Figure3_6.png","static/Figure3_7.png","static/Figure3_8.png","static/Figure3_9.png","static/Figure3_10.png","static/Figure3_11.png","static/Figure3_12.png","static/Figure3_13.png","static/Figure3_14.png","static/Figure3_15.png","static/Figure3_16.png","static/Figure3_17.png","static/Figure3_18.png","static/Figure3_19.png","static/Figure3_20.png","static/Figure3_21.png","static/Figure3_22.png","static/Figure3_23.png","static/Figure3_24.png","static/Figure3_25.png","static/Figure3_26.png","static/Figure3_27.png","static/Figure3_28.png","static/Figure3_29.png","static/Figure3_30.png","static/Figure3_31.png","static/Figure3_32.png"],"names":["breakpoints","fontSizes","colors","black","grey","lightgrey","yellow","green","lightgreen","space","fonts","body","heading","monospace","fontWeights","bold","lineHeights","shadows","small","large","variants","text","buttons","primary","color","bg","Module","imagePrefix","index","sections","imageCaptions","reduce","acc","val","image","imageCaption","py","theme","fontFamily","fontWeight","fontSize","display","some","_","pr","mr","sx","borderRight","toLowerCase","map","section","imageSuffix","indexOf","undefined","Section","imageDescription","imageWidth","LabelledImage","flexDirection","justifyContent","mt","mb","height","width","src","textAlign","length","source","alignItems","maxWidth","t","Introduction","Figure1_1","Methodology","Figure2_1","Figure2_2","Figure2_3","Figure2_4","Figure2_5","modules","Figure3_1","Figure3_2","Figure3_3","Figure3_4","Figure3_5","Figure3_6","Figure3_7","Figure3_8","Figure3_9","Figure3_10","Figure3_11","Figure3_12","Figure3_13","Figure3_14","Figure3_15","Figure3_16","Figure3_17","Figure3_18","Figure3_19","Figure3_20","Figure3_21","Figure3_22","Figure3_23","Figure3_24","Figure3_25","Figure3_26","Figure3_27","Figure3_28","Figure3_29","Figure3_30","Figure3_31","Figure3_32","App","px","pt","pb","module","id","Boolean","window","location","hostname","match","ReactDOM","render","StrictMode","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message","exports"],"mappings":"6MAAe,GACbA,YAAa,CAAC,OAAQ,OAAQ,QAC9BC,UAAW,CAAC,GAAI,GAAI,GAAI,GAAI,GAAI,GAAI,GAAI,IACxCC,OAAQ,CACNC,MAAO,OACPC,KAAM,UACNC,UAAW,UACXC,OAAQ,UACRC,MAAO,UACPC,WAAY,WAEdC,MAAO,CAAC,EAAG,EAAG,EAAG,GAAI,GAAI,GAAI,IAAK,KAClCC,MAAO,CACLC,KAAM,wBACNC,QAAS,aACTC,UAAW,oBAEbC,YAAa,CACXH,KAAM,IACNC,QAAS,IACTG,KAAM,KAERC,YAAa,CACXL,KAAM,IACNC,QAAS,MAEXK,QAAS,CACPC,MAAO,8BACPC,MAAO,gCAETC,SAAU,GACVC,KAAM,GACNC,QAAS,CACPC,QAAS,CACPC,MAAO,QACPC,GAAI,aCrBGC,EAAgC,SAAC,GAKvC,IAJLd,EAII,EAJJA,QACAe,EAGI,EAHJA,YACAC,EAEI,EAFJA,MACAC,EACI,EADJA,SAGMC,EAAgBD,EAASE,QAAiB,SAACC,EAAKC,EAAKL,GAKzD,OAJMK,EAAIC,OAAWD,EAAIE,eACvBH,EAAG,sBAAOA,GAAP,CAAYC,EAAIE,gBAGdH,IACN,IAEH,OAAO,kBAAC,IAAD,CACLI,GAAI,GAEJ,kBAAC,IAAD,CACEZ,MAAOa,EAAMnC,OAAOM,WACpB8B,WAAW,eACXC,WAAW,SACXC,SAAS,OACTC,QAAQ,iBAGL,CAAC,EAAG,EAAG,GAAGC,MAAK,SAAAC,GAAC,OAAIA,IAAMf,MACxB,kBAAC,IAAD,CACDJ,MAAOa,EAAMnC,OAAOG,UACpBoC,QAAQ,eACRG,GAAI,EACJC,GAAI,EACJC,GAAI,CACFC,YAAY,aAAD,OAAeV,EAAMnC,OAAOG,aAEzCuB,GAEHhB,EAAQoC,eAGTnB,EAASoB,KAAI,SAACC,GAAD,OACX,kBAAC,EAAD,iBACMA,EADN,CAEEvB,YAAaA,EACbwB,YAAaD,EAAQf,aAAeL,EAAcsB,QAAQF,EAAQf,cAAgB,OAAIkB,U,iBCvCnFC,EAAkC,SAAC,GAQzC,IAPLpB,EAOI,EAPJA,MACAC,EAMI,EANJA,aACAoB,EAKI,EALJA,iBACA5B,EAII,EAJJA,YACAwB,EAGI,EAHJA,YACAK,EAEI,EAFJA,WACAnC,EACI,EADJA,KAEMoC,EAAgB,kBACpB,kBAAC,IAAD,CACEC,cAAc,SACdC,eAAe,SACfC,GAAI,EACJC,GAAI,EACJC,OAAO,OACPC,MAAO,CAAC,OAAQP,GAAc,SAE9B,kBAAC,IAAD,CACEQ,IAAK9B,EACLW,GAAI,CAAC,EAAG,EAAG,KAEb,kBAAC,IAAD,CACErB,MAAOa,EAAMnC,OAAOE,KACpBoC,SAAS,SACToB,GAAI,EACJC,GAAI,EACJI,UAAU,UALZ,IAME,+CAAiBtC,EAAjB,YAAgCwB,EAAhC,MAAuDhB,GACzD,kBAAC,IAAD,CACEX,MAAOa,EAAMnC,OAAOE,KACpBoC,SAAS,QACToB,GAAI,EACJK,UAAWV,GAAoBA,EAAiBW,OAAS,IAAM,OAAS,UAExE,kBAAC,IAAD,CACEC,OAAQZ,OAMhB,OAAO,kBAAC,IAAD,CACLa,WAAclC,EAAQ,SAAW,aACjCwB,cAAc,SACdW,SAAS,QACTN,MAAM,UAGF7B,GACC,kBAACuB,EAAD,QAIDpC,GAAQA,EAAK4B,KAAI,SAAAqB,GAAC,OAClB,kBAAC,IAAD,CAAeH,OAAQG,S,iBCtElBC,EAAwB,CACnC3D,QAAS,eACTe,YAAa,IACbE,SAAU,CACR,CACER,KAAM,CACJ,ibACA,uBACA,klBAGJ,CACEa,M,OAAOsC,EACPrC,aAAc,sCACdoB,iBACE,oNACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,gCACA,4PACA,qvBACA,8eAGJ,CACEA,KAAM,CACJ,sCACA,mcACA,0kBACA,mc,qFC1BKoD,EAAuB,CAClC7D,QAAS,cACTe,YAAa,IACbE,SAAU,CACR,CACER,KAAM,CACJ,2IACA,4CACA,0WACA,0bACA,0MAGA,yfAGJ,CACEa,MAAOwC,IACPvC,aAAc,qBACdoB,iBAAkB,sCAClBC,WAAY,OAEd,CACEtB,MAAOyC,IACPxC,aAAc,iBACdoB,iBAAkB,kCAClBC,WAAY,OAEd,CACEnC,KAAM,CACJ,4YACA,saAGJ,CACEa,MAAO0C,IACPzC,aAAc,kCACdoB,iBACE,uIACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,+XACA,8LAIJ,CACEa,MAAO2C,IACP1C,aAAc,sCAEhB,CACEd,KAAM,CACJ,4CACA,wPACA,oBACA,gdACA,4iBACA,gCACA,msBAGJ,CACEa,MAAO4C,IACP3C,aAAc,+BAEhB,CACEd,KAAM,CACJ,opB,ylBC9DF0D,GAAU,CACdR,EACAE,ECgB8B,CAC9B7D,QAAS,uBACTe,YAAa,IACbE,SAAU,CACR,CACER,KAAM,CACJ,4CACA,yNAGJ,CACEa,MAAO8C,IACP7C,aAAc,0CACdoB,iBACE,oNACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,kBACA,8OAGJ,CACEa,MAAO+C,IACP9C,aAAc,oCACdoB,iBACE,qFACFC,WAAY,OAEd,CACEtB,MAAOgD,IACP/C,aAAc,8BACdoB,iBACE,oHACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,oLAGJ,CACEa,MAAOiD,IACPhD,aAAc,oCACdoB,iBACE,qFACFC,WAAY,OAEd,CACEtB,MAAOkD,IACPjD,aAAc,8BACdoB,iBACE,wHACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,wEAGJ,CACEa,MAAOmD,IACPlD,aAAc,qBACdoB,iBAAkB,6CAClBC,WAAY,OAEd,CACEnC,KAAM,CACJ,sZACA,0BACA,kbAGJ,CACEa,MAAOoD,IACPnD,aAAc,oCACdoB,iBACE,8GACFC,WAAY,OAEd,CACEtB,MAAOqD,IACPpD,aAAc,8BACdoB,iBACE,8GACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,2aACA,6CACA,+PACA,wiBAGJ,CACEa,MAAOsD,IACPrD,aAAc,uBACdoB,iBACE,gGACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,oLACA,iBAGJ,CACEa,MAAOuD,IACPtD,aAAc,2CACdqB,WAAY,OAEd,CACEtB,MAAOwD,IACPvD,aACE,uFACFqB,WAAY,OAEd,CACEnC,KAAM,CACJ,wVAGJ,CACEa,MAAOyD,IACPxD,aACE,uFACFqB,WAAY,OAEd,CACEtB,MAAO0D,IACPzD,aAAc,6CACdqB,WAAY,OAEd,CACEtB,MAAO2D,KACP1D,aAAc,uDACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,0QAGJ,CACEa,MAAO4D,KACP3D,aAAc,uDACdoB,iBACE,6HACFC,WAAY,OAEd,CACEtB,MAAO6D,KACP5D,aAAc,yDACdoB,iBACE,6GACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,6QAGJ,CACEa,MAAO8D,KACP7D,aAAc,sCACdoB,iBACE,qFACFC,WAAY,OAEd,CACEtB,MAAO+D,KACP9D,aAAc,mCACdoB,iBACE,iEACFC,WAAY,OAEd,CACEnC,KAAM,CACJ,4CACA,4BACA,kcACA,4qBAGJ,CACEa,MAAOgE,KACP/D,aAAc,+BACdqB,WAAY,OAEd,CACEtB,MAAOiE,KACPhE,aAAc,qCACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,8UACA,gfACA,6VACA,oMAGJ,CACEa,MAAOkE,KACPjE,aAAc,uCAEhB,CACED,MAAOmE,KACPlE,aAAc,+CACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,6FAGJ,CACEa,MAAOoE,KACPnE,aAAc,uCAEhB,CACED,MAAOqE,KACPpE,aAAc,6CACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,kHAGJ,CACEa,MAAOsE,KACPrE,aAAc,iCACdqB,WAAY,OAEd,CACEtB,MAAOuE,KACPtE,aAAc,0CACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,gCACA,oJAGJ,CACEa,MAAOwE,KACPvE,aAAc,iDACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,+GAGJ,CACEa,MAAOyE,KACPxE,aAAc,+CACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,8SACA,qdAGJ,CACEa,MAAO0E,KACPzE,aAAc,wCAEhB,CACED,MAAO2E,KACP1E,aAAc,gDACdqB,WAAY,OAEd,CACEnC,KAAM,CACJ,msBAGJ,CACEa,MAAO4E,KACP3E,aAAc,sCAEhB,CACED,MAAO6E,KACP5E,aAAc,8CACdqB,WAAY,SCnUiB,CACjC5C,QAAS,aACTiB,SAAU,CACR,CACER,KAAM,CACJ,kMACA,qCACA,yhBACA,6XACA,4CACA,uXACA,8ZACA,ieACA,oOACA,0eACA,wPACA,8BACA,0XACA,2dACA,iPACA,iTACA,ucACA,yQCtB2B,CACjCT,QAAS,aACTiB,SAAU,CACR,CACER,KAAM,CACJ,sgCCLwB,CAC9BT,QAAS,UACTiB,SAAU,CACR,CACER,KAAM,CACJ,4iCJiEO2F,GA/CH,WACV,OAAO,kBAAC,IAAD,CAAe3E,MAAOA,GAC3B,kBAAC,IAAD,CAAK4E,GAAI,GACP,kBAAC,IAAD,CACEzF,MAAOa,EAAMnC,OAAOK,MACpB+B,WAAW,eACXE,SAAS,OACTD,WAAW,SACX2E,GAAI,EACJC,GAAI,GANN,mCAUA,kBAAC,IAAD,CACE3F,MAAOa,EAAMnC,OAAOE,KACpBoC,SAAS,QACT2E,GAAI,GAHN,4EAMCpC,GAAQ9B,KAAI,SAACmE,EAAQxF,GAAT,OACX,kBAAC,EAAD,eACEA,MAAOA,GACHwF,QAKV,yBAAKC,GAAG,UACN,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,4BACA,+BKvDcC,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2DCbNC,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,GAAD,OAEFC,SAASC,eAAe,SD0HpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,a,mBEzI5BrB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,uC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,iBCA3CtB,EAAOsB,QAAU,sxL,mBCAjBtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC,mBCA3CtB,EAAOsB,QAAU,IAA0B,wC","file":"static/js/main.9a9662ce.chunk.js","sourcesContent":["export default {\n  breakpoints: [\"40em\", \"52em\", \"64em\"],\n  fontSizes: [12, 14, 16, 20, 24, 32, 48, 64],\n  colors: {\n    black: \"#000\",\n    grey: \"#838389\",\n    lightgrey: \"#E5E5E5\",\n    yellow: \"#FFBD41\",\n    green: \"#4A7856\",\n    lightgreen: \"#C8D5B9\"\n  },\n  space: [0, 4, 8, 16, 32, 64, 128, 256],\n  fonts: {\n    body: \"system-ui, sans-serif\",\n    heading: \"sans-serif\",\n    monospace: \"Menlo, monospace\"\n  },\n  fontWeights: {\n    body: 400,\n    heading: 700,\n    bold: 700\n  },\n  lineHeights: {\n    body: 1.5,\n    heading: 1.25\n  },\n  shadows: {\n    small: \"0 0 4px rgba(0, 0, 0, .125)\",\n    large: \"0 0 24px rgba(0, 0, 0, .125)\"\n  },\n  variants: {},\n  text: {},\n  buttons: {\n    primary: {\n      color: \"white\",\n      bg: \"primary\"\n    }\n  }\n};\n","import React from 'react'\nimport { Box, Heading, Text } from 'rebass'\nimport { Section } from \"./\"\nimport { SectionProps as TSection } from './Section'\nimport theme from '../theme'\n\nexport type ModuleProps = {\n  heading: string\n  index?: number\n  imagePrefix?: string\n  sections: TSection[]\n}\n\n\nexport const Module: React.FC<ModuleProps> = ({\n  heading,\n  imagePrefix,\n  index,\n  sections\n}) => {\n\n  const imageCaptions = sections.reduce<string[]>((acc, val, index) => {\n    if (!!val.image && !!val.imageCaption) {\n      acc = [...acc, val.imageCaption]\n    }\n\n    return acc\n  }, [])\n\n  return <Box\n    py={3}\n  >\n    <Heading\n      color={theme.colors.lightgreen}\n      fontFamily=\"Nanum Gothic\"\n      fontWeight=\"normal\"\n      fontSize=\"3rem\"\n      display=\"inline-block\"\n    >\n      {\n        ![0, 4, 5].some(_ => _ === index)\n        && <Text\n          color={theme.colors.lightgrey}\n          display=\"inline-block\"\n          pr={2}\n          mr={3}\n          sx={{\n            borderRight: `1px solid ${theme.colors.lightgrey}`\n          }}\n        >{index}</Text>\n      }\n      {heading.toLowerCase()}</Heading>\n\n    {\n      sections.map((section: TSection) =>\n        <Section\n          {...section}\n          imagePrefix={imagePrefix}\n          imageSuffix={section.imageCaption ? imageCaptions.indexOf(section.imageCaption) + 1 : undefined}\n        />\n      )}\n  </Box>\n}","import React from 'react'\nimport { Box, Flex, Heading, Image, Text } from 'rebass'\nimport ReactMarkdown from 'react-markdown'\nimport theme from '../theme'\nimport { ModuleProps as TModule } from './Module'\n\nexport type SectionProps = {\n  image?: string\n  imageCaption?: string\n  imagePrefix?: TModule[\"imagePrefix\"]\n  imageSuffix?: number\n  imageWidth?: string\n  imageDescription?: string\n  list?: string[]\n  subtitle?: string\n  text?: string[]\n  title?: string\n}\n\nexport const Section: React.FC<SectionProps> = ({\n  image,\n  imageCaption,\n  imageDescription,\n  imagePrefix,\n  imageSuffix,\n  imageWidth,\n  text\n}) => {\n  const LabelledImage = () =>\n    <Flex\n      flexDirection=\"column\"\n      justifyContent=\"center\"\n      mt={4}\n      mb={3}\n      height=\"auto\"\n      width={['100%', imageWidth || \"100%\"]}\n    >\n      <Image\n        src={image}\n        mr={[0, 0, 2]}\n      />\n      <Text\n        color={theme.colors.grey}\n        fontSize=\"0.75em\"\n        mt={3}\n        mb={1}\n        textAlign=\"center\"\n      > <span>{`Figure ${imagePrefix}.${imageSuffix}:`}</span>{imageCaption}</Text>\n      <Text\n        color={theme.colors.grey}\n        fontSize=\"0.8em\"\n        mt={1}\n        textAlign={imageDescription && imageDescription.length > 100 ? \"left\" : \"center\"}\n      >\n        <ReactMarkdown\n          source={imageDescription}\n        />\n      </Text>\n    </Flex>\n\n\n  return <Flex\n    alignItems={!!image ? \"center\" : \"flex-start\"}\n    flexDirection=\"column\"\n    maxWidth=\"40rem\"\n    width=\"100%\"\n  >\n    {\n      !!image\n      && <LabelledImage />\n    }\n\n    {\n      !!text && text.map(t =>\n        <ReactMarkdown source={t} />\n      )\n    }\n\n  </Flex>\n}\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nimport Figure1_1 from \"../static/Figure1_1.png\";\n\nexport const Introduction: TModule = {\n  heading: \"Introduction\",\n  imagePrefix: \"1\",\n  sections: [\n    {\n      text: [\n        \"Successful crop cultivation depends on weed control during the initial weeks of planting. Recently, area-saturation pesticide usage preferentially supplanted mechanical weed control but failed to completely eliminate weed problems \\\\[5\\\\]. As such, site-specific pesticide application is needed to reduce overall pesticide usage \\\\[8\\\\]. Thus, rapidly identifying sparsely-distributed weeds is crucial for the agricultural sector.\",\n        \"## 🌱 Data\",\n        \"Researchers from University of Southern Denmark and Aarhus University publicly released a dataset \\\\[4\\\\] to facilitate ML-based weed classification during nascent phases and intervention prior to their negative influence on cash crop growth. The dataset comprises 5, 539 images of seedlings grown in consistent indoor conditions, taken at 2-3 day intervals over a 20-day period, labelled by species (3 crop, 9 weed species), and with a resolution of 10 pixel-per-millimeter. The dataset has been published on [Kaggle](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset/kernels).\"\n      ]\n    },\n    {\n      image: Figure1_1,\n      imageCaption: \"12 Classes of Plants - Input Images\",\n      imageDescription:\n        \"Black-Grass, Charlock, Cleavers, Common Chickweed, Common Wheat, Fat Hen, Loose Silky-Bent, Maize, Scentless Mayweed, Shepherd’s Purse, Small-Flowered Cranesbill, Sugar Beet (left to right; top to bottom)\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"## 🌱 Previous work\",\n        \"Convolutional Neural Networks (CNNs) have been shown to be ideally suited for image classification tasks. Since the publication of the dataset on Kaggle, there have been several attempts at using CNNs to classify images from this dataset by species.\",\n        \"Alimboyong et. al. from the Technological Institute of the Philippines used extensive data augmentation to increase the size of the dataset to 118,750 images. 83,126 of these were applied to train a CNN with an AlexNet architecture through transfer learning from weights initialized on the ImageNet dataset. This model achieved 99.74% accuracy on the augmented test set. While impressive, this model took 99 hours to train \\\\[1\\\\] and did not includede augmented dataset creation, making this approach infeasible for real-world applications. Since agricultural needs vary widely, a cheaper approach would allow each cultivator to produce a classification model trained on relevant crop and weed species in its target environment and yield better results.\",\n        \"Kaggle user “Marsh” takes the opposite approach. Citing the class imbalance present in the dataset, they seek to train an effective model by downsampling the over-represented plant species to 250 images per species. A custom architecture was used with randomly initialized weights. After 40 training epochs, this model correctly classified 79% of the test set. The smaller training set and light-weight architecture resulted in a drastically shorter training time of 400 seconds.\"\n      ]\n    },\n    {\n      text: [\n        \"## 🌱 Research objectives\",\n        \"Generally, the current practice for weed identification uses a CNN to classify images. The quality of the produced model depends heavily on the quality of the dataset. Kaggle user Allunia \\\\[2\\\\] highlights the danger of applying a model without examining how the model has extracted knowledge, especially when the dataset is artificial and standardized. The lack of variation in our raw dataset may yield a CNN model that does not generalize well.\",\n        \"In particular, target leakage of background pixels is a primary concern. Every seedling image is taken against a background of pebbles and/or a measuring stick. Different species of seedlings have different sizes at the same growth stage, meaning that pictures of smaller species will have a higher zoom level. A CNN can use the relative scale of the pebbles or measuring stick to classify the seedlings instead of using the features of the seedlings themselves. This is a problem because test datasets in the real world may have the seedlings against different types of backgrounds.\",\n        \"Another issue with this dataset is an imbalance of class sizes. There are three crop species compared to nine weed species, and the number of images per species ranges from 253 to 762. The least represented species is common wheat, a cash crop whose correct classification is essential for industry. We add significant contributions to a combination of previous works to address these problems and produce a more accurate and generalized model.\"\n      ]\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nimport Figure2_1 from \"../static/Figure2_1.png\";\nimport Figure2_2 from \"../static/Figure2_2.png\";\nimport Figure2_3 from \"../static/Figure2_3.jpg\";\nimport Figure2_4 from \"../static/Figure2_4.png\";\nimport Figure2_5 from \"../static/Figure2_5.png\";\nimport { worker } from \"cluster\";\n\nexport const Methodology: TModule = {\n  heading: \"Methodology\",\n  imagePrefix: \"2\",\n  sections: [\n    {\n      text: [\n        \"For this project, we investigated both supervised and unsupervised learning techniques to support the classification of weeds and crops.\",\n        \"## 🌱 Unsupervised segmentation\",\n        \"To address target leakage in prior work, we employed unsupervised learning to segment the plant leaves from the background before training our supervised learners.  Without ground truth values for the plant body within the original dataset, our goals for unsupervised segmentation were to extract clear-enough leaves for passing to the supervised classifiers.\",\n        \"As the target is fairly different from the background, clustering within color-space presented the most intuitive option. Since image color information is a bounded, continuous space, we decided that hard assignment along the color-information (and not spatial) axes would be sufficient and much more performant.  Therefore, we decided upon *k*-means clustering as our segmentation technique.  However, this posed three research questions:\",\n        \"1. Which color space is best to quantize?\\n\" +\n          \"1. What is the optimal number of cluster centers to segment out the plant body?\\n\" +\n          \"1. How do we avoid mislabeling due to reflectance and illumination changes?\",\n        \"For the first research question, we compared plant body segmentation for the quantization of all color channels within RGB-space to the hue channel within HSV-space. To do so, we used the elbow method \\\\[9\\\\] to generate a putative range of the number of clusters for each space, and then visually confirmed that segmentation would produce a cluster centroid corresponding to a plant body. Furthermore, we compared runtimes to determine which method was more performant, both in runtime and accuracy.\"\n      ]\n    },\n    {\n      image: Figure2_1,\n      imageCaption: \"HSV Space Cylinder\",\n      imageDescription: \"The HSV model mapped to a cylinder.\",\n      imageWidth: \"40%\"\n    },\n    {\n      image: Figure2_2,\n      imageCaption: \"RGB Space Cube\",\n      imageDescription: \"The RGB model mapped to a cube.\",\n      imageWidth: \"40%\"\n    },\n    {\n      text: [\n        \"In addition, there exists the need to automatically determine the optimal number of clusters upon which to quantize hue information in order to facilitate rapid segmentation.  To do so, we utilized the putative range of cluster numbers from the elbow method to calculate the average silhouette value for all clusters, retaining the number of clusters with the maximum average silhouette value.\",\n        \"However, hue-quantization presents the additional challenge of illumination changes affecting diffuse reflection of green hues off of white surfaces (Figure 2.3).  To resolve this, we investigated two processes: (1) thresholding, which is a pre-process that removes all near-white pixels before segmentation, and (2) cascading segmentation, which implements two rounds of segmentation within alternating color spaces.\"\n      ]\n    },\n    {\n      image: Figure2_3,\n      imageCaption: \"Diffuse and Specular Reflection\",\n      imageDescription:\n        \"In HSV space, white surfaces can have a green hue with low saturation, particularly if the white surface is near a green plant body.\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"Once our images are pre-processed, we can implement *k*-means clustering upon just the relevant channels.  The resultant image can be processed with non-green suppression to extract the pixels corresponding to the plant body before passing the processed images to the supervised learners. In case multiple “green” hues are retained, we implemented two different methods:\",\n        \"1. MaxGreen, which segments according to the most common green hue found in the image, **or**\\n\" +\n          \"1. AllGreen, which segments according to all possible green hues found as cluster centers.\"\n      ]\n    },\n    {\n      image: Figure2_4,\n      imageCaption: \"Unsupervised Segmentation Pipeline\"\n    },\n    {\n      text: [\n        \"## 🌱 Supervised classification\",\n        \"Given labels for the input images, as well as segmented versions of the images, we wished to build classification models to distinguish both between weeds and crops.  We compared a random forest to a neural network, which we will describe below.\",\n        \"### Random forest\",\n        \"Since quickly training models is a desirable feature of the model, it is important to research the performance difference in classification between random forest and neural networks.  In the investigation, random forest models were created for both binary classification between weeds and crops, as well as classification between the comprehensive list of labels.  Since images were segmented through *k*-means, models were also trained on the segmented images.\",\n        \"In order to train the random forest model, features need to be extracted that differentiate between images. Pixel color histograms were generated for the color images, as well as pixel intensities for grayscale images.  Bin values were then extracted as linear features.  In addition, multidimensional histograms using AND logic were created for the red, green, and blue-values as well:  One for all three together and two for red/green and blue/green.  For example, one bin would be for how many pixels have red, green and blue values between 0 and 10.\",\n        \"### Neural network classifier\",\n        \"We aim to develop a CNN image classifier that yields accurate results without a resource or time-intensive training process; all while dealing with the issues inherent in this dataset. To address the class imbalance, we create a balanced dataset by undersampling all classes down to 253 images per species. We create a training set and a testing set using an 80/20 split. We use 80% of the training set for training and 20% for validation. These splits yield 1948 training images, 488 validation images, and 600 testing images. To compensate for this relatively small dataset, we use the Xception model architecture \\\\[3\\\\] and use initial weights that have been trained on the ImageNet dataset\\\\[6\\\\].\"\n      ]\n    },\n    {\n      image: Figure2_5,\n      imageCaption: \"Xception Model Architecture\"\n    },\n    {\n      text: [\n        \"The model was trained using the Adam optimizer with an initial learning rate of 0.0001 for 60 epochs. Training data was fed to the model in batches of 32 images. After each epoch, the validation accuracy was calculated. If the validation accuracy did not improve for three consecutive epochs, the learning rate was reduced by a factor of ⅕ with a minimum possible learning rate of 0.000001. If the validation accuracy did not improve for 10 consecutive epochs, training was ended and the model was saved with the weights that yielded the highest validation accuracy. This training procedure was repeated to train a model using the segmented images.\"\n      ]\n    }\n  ]\n};\n","import React from 'react';\nimport { ThemeProvider } from 'emotion-theming'\nimport { Heading, Text, Box } from 'rebass'\nimport { Module } from './components'\nimport \"./App.css\"\nimport theme from './theme'\nimport {\n  Introduction,\n  Methodology,\n  Results,\n  Discussion,\n  Conclusion,\n  Sources\n} from './content'\n\n\nconst modules = [\n  Introduction,\n  Methodology,\n  Results,\n  Discussion,\n  Conclusion,\n  Sources,\n] as const\n\nconst App = () => {\n  return <ThemeProvider theme={theme}>\n    <Box px={5}>\n      <Heading\n        color={theme.colors.green}\n        fontFamily=\"Nanum Gothic\"\n        fontSize=\"4rem\"\n        fontWeight=\"normal\"\n        pt={4}\n        pb={2}\n      >\n        50 shades of green 🌿\n      </Heading>\n      <Text\n        color={theme.colors.grey}\n        fontSize=\"0.8em\"\n        pb={5}\n      >Samuel Elkind, Ollie Hsieh, Michael Liang, Nick Martucci, Matt Redington</Text>\n\n      {modules.map((module, index) =>\n        <Module\n          index={index}\n          {...module}\n        />\n      )}\n    </Box>\n\n    <div id=\"leaves\">\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n      <i></i>\n    </div>\n  </ThemeProvider >\n}\n\nexport default App;\n","import { ModuleProps as TModule } from \"../components/Module\";\nimport Figure3_1 from \"../static/Figure3_1.png\";\nimport Figure3_2 from \"../static/Figure3_2.png\";\nimport Figure3_3 from \"../static/Figure3_3.png\";\nimport Figure3_4 from \"../static/Figure3_4.png\";\nimport Figure3_5 from \"../static/Figure3_5.png\";\nimport Figure3_6 from \"../static/Figure3_6.png\";\nimport Figure3_7 from \"../static/Figure3_7.png\";\nimport Figure3_8 from \"../static/Figure3_8.png\";\nimport Figure3_9 from \"../static/Figure3_9.png\";\nimport Figure3_10 from \"../static/Figure3_10.png\";\nimport Figure3_11 from \"../static/Figure3_11.png\";\nimport Figure3_12 from \"../static/Figure3_12.png\";\nimport Figure3_13 from \"../static/Figure3_13.png\";\nimport Figure3_14 from \"../static/Figure3_14.png\";\nimport Figure3_15 from \"../static/Figure3_15.png\";\nimport Figure3_16 from \"../static/Figure3_16.png\";\nimport Figure3_17 from \"../static/Figure3_17.png\";\nimport Figure3_18 from \"../static/Figure3_18.png\";\nimport Figure3_19 from \"../static/Figure3_19.png\";\nimport Figure3_20 from \"../static/Figure3_20.png\";\nimport Figure3_21 from \"../static/Figure3_21.png\";\nimport Figure3_22 from \"../static/Figure3_22.png\";\nimport Figure3_23 from \"../static/Figure3_23.png\";\nimport Figure3_24 from \"../static/Figure3_24.png\";\nimport Figure3_25 from \"../static/Figure3_25.png\";\nimport Figure3_26 from \"../static/Figure3_26.png\";\nimport Figure3_27 from \"../static/Figure3_27.png\";\nimport Figure3_28 from \"../static/Figure3_28.png\";\nimport Figure3_29 from \"../static/Figure3_29.png\";\nimport Figure3_30 from \"../static/Figure3_30.png\";\nimport Figure3_31 from \"../static/Figure3_31.png\";\nimport Figure3_32 from \"../static/Figure3_32.png\";\n\nexport const Results: TModule = {\n  heading: \"Results and Analysis\",\n  imagePrefix: \"3\",\n  sections: [\n    {\n      text: [\n        \"## 🌱 Unsupervised segmentation\",\n        \"In general, unsupervised segmentation produced high-quality results. In the following sections, we will go over the specific results of our experiments, as well as discuss our strategies for problematic outliers.\"\n      ]\n    },\n    {\n      image: Figure3_1,\n      imageCaption: \"12 Classes of Plants - Segmented Images\",\n      imageDescription:\n        \"Black-Grass, Charlock, Cleavers, Common Chickweed, Common Wheat, Fat Hen, Loose Silky-Bent, Maize, Scentless Mayweed, Shepherd’s Purse, Small-Flowered Cranesbill, Sugar Beet (left to right; top to bottom)\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"### RGB vs. HSV\",\n        \"Initial naive clustering within RGB-space allowed for determining the optimal number of centers using the elbow method and resulted in a consistent range of cluster numbers k=\\\\[3,6\\\\] for all weeds and cash crops within the dataset.\"\n      ]\n    },\n    {\n      image: Figure3_2,\n      imageCaption: \"Elbow Method for RGB Quantization\",\n      imageDescription:\n        \"These representative results indicate an optimal *k*-value in the range \\\\[3,6\\\\].\",\n      imageWidth: \"60%\"\n    },\n    {\n      image: Figure3_3,\n      imageCaption: \"Results of RGB Quantization\",\n      imageDescription:\n        \"The loss of the plant body and confusion with the background consistently occurs for plant bodies that are sparse\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"Similarly, applying the elbow method to hue-quantization within HSV-space produced the same number of cluster center optima as with RGB-space, but with much lower loss values.\"\n      ]\n    },\n    {\n      image: Figure3_4,\n      imageCaption: \"Elbow Method for Hue Quantization\",\n      imageDescription:\n        \"These representative results indicate an optimal *k*-value in the range \\\\[3,6\\\\].\",\n      imageWidth: \"60%\"\n    },\n    {\n      image: Figure3_5,\n      imageCaption: \"Results of Hue Quantization\",\n      imageDescription:\n        \"The plant body is retained, as the hue information for the plant body is more likely to be a centroid for clustering.\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"We also measured the time performance of both quantization methods.\"\n      ]\n    },\n    {\n      image: Figure3_6,\n      imageCaption: \"RGB/HSV Time Ratio\",\n      imageDescription: \"HSV quantization is multiple times faster.\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"Segmentation using the “optimal” k-range for RGB-quantization assigns the plant body to background centroids for images in which plant body pixels are sparse. This is particularly egregious for “grass”-type plant bodies (Figure 3.2). However, hue-quantization doesn’t suffer from these issues (Figure 3.3). Therefore, we decided to primarily use hue-quantization moving forward.\",\n        \"### Finding optimal *k*\",\n        \"In order to determine the best *k*-value for each image, we calculate the silhouette score of each pixel with respect to its cluster label and determine the average over all observations. This was conducted with respect to the hue feature. Then, we pick the number of clusters associated with the silhouette score closest to 1. In Figures 3.6 and 3.7, we plot each observation’s silhouette value within its assigned cluster.\"\n      ]\n    },\n    {\n      image: Figure3_7,\n      imageCaption: \"Representative Weed (Black Grass)\",\n      imageDescription:\n        \"Silhouette analysis of the image indicates a sparse plant body within the image with a best *k*-value of 3.\",\n      imageWidth: \"70%\"\n    },\n    {\n      image: Figure3_8,\n      imageCaption: \"Representative Crop (Maize)\",\n      imageDescription:\n        \"Silhouette analysis of the image indicates a sparse plant body within the image with a best *k*-value of 4.\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"We can see that both the shape of the curve, as well as the average value, correspond to the quality of the cluster assignment. This process requires downsampling the image to reasonable observation size (n <= 10&#x2074;), but leads to reasonable performance prior to segmentation. We can qualitatively assess the validity of our cluster number determination with segmentation results implemented with non-green suppression.\",\n        \"### Thresholding and cascaded segmentation\",\n        \"With the initial set of results from *k*-means, segmented images had varying levels of background and noise present. Given that *k*-means converges toward local minima, we employ thresholding to mitigate potential assignment of white pixels as centers.\",\n        \"Determined through tuning representative samples of 15 then 50 images, the white threshold values ranged from \\\\[120, 120, 120\\\\] to \\\\[180, 180, 180\\\\] and were suppressed as black pixels. Values chosen were largely influenced by illuminance, hue similarity between the background and leaves, and qualitative results from the representative samples. One might have assumed that the values were closer to \\\\[255, 255, 255\\\\] but as brightness is perceived in a relative fashion, the threshold optima point to underexposure or low light conditions.\"\n      ]\n    },\n    {\n      image: Figure3_9,\n      imageCaption: \"Sample (Black-grass)\",\n      imageDescription:\n        \"Original image, thresholding, initial *k*-means, *k*-means after thresholding (left to right)\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"As shown in Figure 3.8, thresholding effectively reduced problematic background pixels, particularly white pixels susceptible to adoption of hue characteristics via reflectance.\",\n        \"### Problems\"\n      ]\n    },\n    {\n      image: Figure3_10,\n      imageCaption: \"Background leakage due to hue similarity\",\n      imageWidth: \"70%\"\n    },\n    {\n      image: Figure3_11,\n      imageCaption:\n        \"Absence of plant pixels due to hue similarity; inaccurate plant pixel identification\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"Other negative factors stemmed from image conditions, such as low light, underexposure, and blurriness from misfocus or poor image quality. The former two led to image clipping (loss of detail) whereas the latter produced masks with ridged edges and opened models to misinterpretation of background pixels with plant-like hues as features.\"\n      ]\n    },\n    {\n      image: Figure3_12,\n      imageCaption:\n        \"Sugar beet - clipping from low light/underexposure; loss of features from blurriness\",\n      imageWidth: \"70%\"\n    },\n    {\n      image: Figure3_13,\n      imageCaption: \"Cleavers - ridged edges due to blurriness \",\n      imageWidth: \"70%\"\n    },\n    {\n      image: Figure3_14,\n      imageCaption: \"Maize - ridged edges due to low light / low contrast\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"Considerations for improving segmentation results included normalizing image values and evaluating the ratio of plant to background pixels. However the inconsistency in image conditions and presence of features ultimately led to abandonment of these approaches.\"\n      ]\n    },\n    {\n      image: Figure3_15,\n      imageCaption: \"Maize - ridged edges due to low light / low contrast\",\n      imageDescription:\n        \"Blurry, zoomed-in, with container; blurry, zoomed-in, inter-plant occlusion;\\nzoomed-out, dense foliage; blurry, zoomed-in\",\n      imageWidth: \"70%\"\n    },\n    {\n      image: Figure3_16,\n      imageCaption: \"Black-grass - variance in image content and conditions\",\n      imageDescription:\n        \"Zoomed-out, with white ruler; blurry, zoomed-in;\\nsparse plant pixels, with white ruler; blurry, zoomed-in\",\n      imageWidth: \"70%\"\n    },\n    {\n      text: [\n        \"Similarly, an attempt to run images through two passes of *k*-means yielded mixed results. As shown in Figure 3.17 and 3.18, this approach proved capable of both successfully isolating plant pixels and faring worse than a single *k*-means pass without suppression.\"\n      ]\n    },\n    {\n      image: Figure3_17,\n      imageCaption: \"Cascading Segmentation: Black Grass\",\n      imageDescription:\n        \"Hue-quantization, followed by RGB-quantization successfully suppressed background.\",\n      imageWidth: \"65%\"\n    },\n    {\n      image: Figure3_18,\n      imageCaption: \"Cascading Segmentation: Cleavers\",\n      imageDescription:\n        \"Hue-quantization, followed by RGB-quantization produced holes.\",\n      imageWidth: \"65%\"\n    },\n    {\n      text: [\n        \"## 🌱 Supervised classification\",\n        \"### Random forest results\",\n        \"For the random forest model we created features of histograms. Each bin of each histogram then becomes a feature. We created different histograms of the red, green, blue and grayscale values. Four one dimensional histograms: one for each of the RGB channels and one for the grayscale channel. Then three multidimensional histograms are created: one for all three channels of RGB, one for the red/green channels and one for the blue/green channels.\",\n        \"Feature creation results in 2688 bins, most of which will not provide any information for the model, requiring a reduction in feature space. The top 200 features are selected using the ANOVA F-value between the label and the feature for a classification task. Due to the features just being different bins in different histograms, it is hard to distinguish which 200 bins are used because the function does not return anything that is labeled. The model itself will also return values for the importance of features but due to not being able to keep track of which bins are being used this information is not valuable. It takes approximately 9.22 seconds to extract the features.\"\n      ]\n    },\n    {\n      image: Figure3_19,\n      imageCaption: \"Representative RGB Histogram\",\n      imageWidth: \"50%\"\n    },\n    {\n      image: Figure3_20,\n      imageCaption: \"Representative Grayscale Histogram\",\n      imageWidth: \"50%\"\n    },\n    {\n      text: [\n        \"For the normal images and the segmented images, there are 203 images from each class used in the training and 50 from each class used in the testing. For the crops and weeds classification there are 606 images in the crops training set, 630 in the weeds training set, 150 in the crops testing set, and 153 in the weeds testing set.\",\n        \"In terms of the hyperparameters for the model, there are 100 trees in the forest, each with a max depth of 25.  The number of features at which to look when considering the best split is the square root of the total number of features. These parameters were chosen to generate the best results from the model. The model was then trained on the dataset in three different ways: with the normal images, with the segmented images, and with the images divided in only two classes, crops and weeds.\",\n        \"The model had an overall precision of 0.84. However there were some standout classes that had a very high precision. The model achieved a 0.96 precision for the common chickweed class and 0.93 for the Shepherd's Purse class, with the lowest precision for the loose silky-bent class at 0.72. It takes approximately 1.31 seconds to train the model.\",\n        \"In terms of time performance, the entire model takes about 1.31 seconds to train. This is predominated by feature extraction time, which totaled 9.22 seconds over the entire domain of images.\"\n      ]\n    },\n    {\n      image: Figure3_21,\n      imageCaption: \"Random Forest (Unsegmented) Metrics\"\n    },\n    {\n      image: Figure3_22,\n      imageCaption: \"Random Forest (Unsegmented) Confusion Matrix\",\n      imageWidth: \"80%\"\n    },\n    {\n      text: [\n        \"In contrast, training upon segmented images resulted in a lowered precision score: 0.77.\"\n      ]\n    },\n    {\n      image: Figure3_23,\n      imageCaption: \"Random Forest (Unsegmented) Metrics\"\n    },\n    {\n      image: Figure3_24,\n      imageCaption: \"Random Forest (Segmented) Confusion Matrix\",\n      imageWidth: \"80%\"\n    },\n    {\n      text: [\n        \"Training a binary classifier to distinguish between weeds and crops yielded much higher precision and recall.\"\n      ]\n    },\n    {\n      image: Figure3_25,\n      imageCaption: \"Random Forest (Binary) Metrics\",\n      imageWidth: \"80%\"\n    },\n    {\n      image: Figure3_26,\n      imageCaption: \"Random Forest (Binary) Confusion Matrix\",\n      imageWidth: \"40%\"\n    },\n    {\n      text: [\n        \"### Neural network classifier\",\n        \"For our neural network classifier, the changes in accuracy and loss over the training process for the unsegmented training and validation sets:\"\n      ]\n    },\n    {\n      image: Figure3_27,\n      imageCaption: \"Neural Network (Unsegmented) Accuracy and Loss\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"The changes in accuracy and loss over the training process for the segmented training and validation sets:\"\n      ]\n    },\n    {\n      image: Figure3_28,\n      imageCaption: \"Neural Network (Segmented) Accuracy and Loss\",\n      imageWidth: \"60%\"\n    },\n    {\n      text: [\n        \"For both models, one epoch took about 63-37 seconds. Consequently, the unsegmented model trained for roughly 40 minutes and the segmented model trained for about 25 minutes. Test images took 366 ms to generate a prediction, so the 600 images in the test set were classified in 3 minutes, 40 seconds.\",\n        \"The model trained on unsegmented images performed very well on the test set. An overall accuracy of 94.7% was achieved. The best classes, Charlock and Small-flowered Cranesbill were classified perfectly, with no false positives nor false negatives. The worst classes, Black-grass and Loose Silky-bent performed similarly with F1-scores of 0.752 and 0.768 respectively. The three cash crops all performed above average with Sugar Beet attaining an F1-score of 0.99.\"\n      ]\n    },\n    {\n      image: Figure3_29,\n      imageCaption: \"Neural Network (Unsegmented) Metrics\"\n    },\n    {\n      image: Figure3_30,\n      imageCaption: \"Neural Network (Unsegmented) Confusion Matrix\",\n      imageWidth: \"80%\"\n    },\n    {\n      text: [\n        \"The model trained on segmented images also performed well on the test set, though not as well as the unsegmented model. An overall accuracy of 91.7% was achieved. Only Small-flowered Cranesbill was classified perfectly, with no false positives nor false negatives. Charlock achieved a perfect precision, but a single false negative led to a recall of 0.98. The worst classes were again Black-grass and Loose Silky-bent with F1-scores of 0.674 and 0.727 respectively. Black-grass suffered from an increase in false negatives while Loose Silky-bent had an increase in false positives. The three cash crops maintained their above average performance with Sugar Beet attaining the highest F1-score of 0.95.\"\n      ]\n    },\n    {\n      image: Figure3_31,\n      imageCaption: \"Neural network (Segmented) Metrics\"\n    },\n    {\n      image: Figure3_32,\n      imageCaption: \"Neural Network (Segmented) Confusion Matrix\",\n      imageWidth: \"80%\"\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nexport const Discussion: TModule = {\n  heading: \"Discussion\",\n  sections: [\n    {\n      text: [\n        \"For the use-case in which fast and accurate responses are crucial for effective application of pesticides, our research focused on developing models that can accurately classify plant bodies.\",\n        \"## 🌱 Image segmentation\",\n        \"Hue-quantized image segmentation provides a fast and effective method of isolating plant bodies from background. Whereas prior work focused on a combination of soft-assignment clustering with Expectation Maximization (EM) and computer vision techniques like generalized Hough transforms and watershed segmentation \\\\[7\\\\], our work produces reliably segmented images at a much faster rate. For over 96% of the images we segmented from the dataset, our results showed clear plant body segmentation and suppression of background pixels.\",\n        \"The quality of the input image heavily impacted the segmentation product. However, any product deployed in the field would be able to be trained upon a consistent set of images, given that the mechanical devices employed would be uniformly similar. Furthermore, in comparison to standard quantization within RGB-space, our method provides some amount of illumination invariance.\",\n        \"## 🌱 Supervised classification\",\n        \"The random forest classifier performs about the same as the published models on this dataset. However it performs worse when using the segmented images. The most potential with this model would be for classifying between crops and weeds because that is where it had its best performance. With further work on the parameters and features, the performance could be enhanced.\",\n        \"The difference in performance between the segmented and unsegmented images shows that there is lost information when the segmentation is performed. The random forest model was more sensitive to this loss, dropping .07 in average precision while the neural networks model only dropped 3%. Perhaps this is due to the features used in the model simply being histograms of the red, green, blue and grayscale values.\",\n        \"The advantage of the random forest model is that its training time is much quicker than the neural networks classifier. However, it takes a serious hit in performance, except in one class. It outperforms neural networks in the Loose Silky-bent class significantly. In the unsegmented dataset, there is a 12% difference in precision. This class somehow benefits from the histogram features and indicates that perhaps a combination of these models could further boost performance.\",\n        \"Both of the neural network classifiers outperform all published models on this unaugmented dataset by a significant margin (to the best of our knowledge). Further hyperparameter tuning would likely yield further improvements.\",\n        \"The juxtaposition of these models clearly demonstrate that the unsegmented model is basing some aspects of its predictions on information present in the background of the images. The background information accounts for a 3% change in overall accuracy and affected every class except Small-flowered Cranesbill. Common Wheat, and Maize were affected particularly strongly. The recall of common wheat predictions dropped from 1.0 to 0.9 and the precision of Maize dropped from 1.0 to 0.904.\",\n        \"Overall, the hyperparameter configuration, downsampled dataset, and transfer learning from weights trained on the ImageNet dataset results in models that strike a good balance between reasonable training times and high classification accuracies.\",\n        \"## 🌱 Future work\",\n        \"While our segmentation pipeline successfully extracts plant body pixels from background on most images, our model could stand to improve upon extracting semantic meaning from the images. For example, while the pipeline produces a single mask upon all plant body pixels, it would be useful to extract individual leaves and stems to determine plant body geometry and structure.\",\n        \"One possible avenue for future work would involve using edge detection, such as with Sobel filters or Canny Edge Detector \\\\[10\\\\], since edge images represent a higher level of abstraction. In addition, edges are feature invariant to absolute illumination. This could reduce overfitting of classifiers to one particular lighting condition. Edge detection in combination with gradient convolutions would allow for calculating relative orientations of the leaves and stems.\",\n        \"Another possible avenue to explore is to cascade-segment upon different features within HSV space, such as saturation values in order to reduce noise. Similarly, noise removal filtering with Wiener filters could also reduce overall noise.\",\n        \"For the random forest classification, further work can be done with feature extraction. The key to a good random forest classifier is good features. The histograms we used are a solid foundation that can be expanded upon. Other measures of the images could be found and used to help differentiate them.\",\n        \"Further work is needed to assess the quality of the CNN-based classifiers. In order to test how well these models generalize, a new test set should be constructed that consists of seedling images of the 12 species that were not taken in the controlled environment of the Aarhaus University labs. The impact of different backgrounds and viewing angles on prediction quality must be considered before applying these classifiers to any practical problems.\",\n        \"One possible avenue to improve model performance for both the unsegmented and segmented models is a cascading classifier. This classifier consists of several trained neural networks that progressively discriminate an image into increasingly granular classes.\"\n      ]\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nexport const Conclusion: TModule = {\n  heading: \"Conclusion\",\n  sections: [\n    {\n      text: [\n        \"Our results confirm that target leakage of background pixels can artificially inflate the accuracy of a model trained on the raw data set. We used unsupervised hue-quantized image segmentation to quickly and effectively produce a data set of seedlings isolated from the background. We then independently trained our supervised learning models with the original data set and the segmented data to compare the results. Both the random forest model and the CNN achieved higher accuracy for the unsegmented images, likely due to consistent background information across the training set and the test set. The models trained with the segmented data set experienced a modest drop in accuracy for this test set, but we anticipate that these models are much better suited to generalize to real-world applications. New, varied test sets are needed to fully assess the CNN-based classifiers, but our results show that unsupervised segmentation is feasible and effective for ensuring a model extracts knowledge from relevant features.\"\n      ]\n    }\n  ]\n};\n","import { ModuleProps as TModule } from \"../components/Module\";\n\nexport const Sources: TModule = {\n  heading: \"Sources\",\n  sections: [\n    {\n      text: [\n        \"1. Alimboyong, C., Hernandez, A., Medina, R. **Classification of Plant Seedling Images Using Deep Learning**\\n\" +\n          \"1. Allunia. **[Train as you fight](https://www.kaggle.com/allunia/train-as-you-fight-with-seedlings)**\\n\" +\n          \"1. Chollet, F. **Xception: Deep Learning with Depthwise Separable Convolutions**\\n\" +\n          \"1. Giselsson, T., Jørgensen, R. Jensen, P., Dyrmann, M., Midtiby, H. **A public image database for benchmark of plant seedling classification algorithms**.\\n\" +\n          \"1. Hock, B., Fedtke, C., Schmidt, R. **Herbizide: Entwicklung, Anwendung, Wirkungen, Nebenwirkungen**\\n\" +\n          '1. **[\"ImageNet\"](http://www.image-net.org/)**\\n' +\n          \"1. Scharr, H., Minverini, M., French, A., Klukas, C., Kramer, D., Liu, X., Luengo, I., Pape, J., Polder, G. Vukadinovic, D., Yin, X., Tsaftaris, S. **Leaf segmentation in plant phenotyping**\\n\" +\n          \"1. Timmermann, C., Gerhards, R., Kühbauch, W. **The  Economic  Impact  of  Site-Specific Weed Control**\\n\" +\n          \"1. Thorndike, R. **Who Belongs in the Family?**\\n\" +\n          \"1. Valliammal, N., Geethalakshmi, S.N. **Plant Leaf Segmentation Using Non Linear K means Clustering**\\n\"\n      ]\n    }\n  ]\n};\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n","module.exports = __webpack_public_path__ + \"static/media/Figure1_1.6e004a1f.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure2_1.7c6685f6.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure2_2.71b0acc8.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure2_3.c9d4c87e.jpg\";","module.exports = __webpack_public_path__ + \"static/media/Figure2_4.f0c9b6e1.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure2_5.81e06827.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_1.d0a81ecb.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_2.a03f9287.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_3.6735d5aa.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_4.d00f8d09.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_5.18ffb136.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_6.dc823682.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_7.d74cf084.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_8.68a4de67.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_9.aee36164.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_10.79081225.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_11.24724826.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_12.4c2ca49d.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_13.cf861fd2.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_14.b9621cdf.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_15.1be00a33.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_16.e1d19f06.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_17.6e7ddfcd.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_18.5cd5af40.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_19.eeb10a26.png\";","module.exports = \"data:image/png;base64,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\"","module.exports = __webpack_public_path__ + \"static/media/Figure3_21.7a88e93f.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_22.03321d51.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_23.a49ccc5d.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_24.5774be06.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_25.037703dd.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_26.fb3b50bc.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_27.d73ede9c.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_28.1e74e6c2.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_29.6ad8e627.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_30.140cd425.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_31.0999da7e.png\";","module.exports = __webpack_public_path__ + \"static/media/Figure3_32.c7896b2f.png\";"],"sourceRoot":""}